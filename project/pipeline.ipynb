{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6q8_gU4RClu"
   },
   "source": [
    "# MNIST Digits Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRRWy7XpRHbq"
   },
   "source": [
    "## Importing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    from tensorflow import keras\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \"\"\"Load MNIST dataset and save train and test data as files in the file system.\"\"\"\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # check shape of the data\n",
    "    print(\"<==== data shape ====>\")\n",
    "    print(f\"x_train: {x_train.shape} || y_train: {y_train.shape}\")\n",
    "    print(f\"x_test: {x_test.shape} || y_test: {y_test.shape}\")\n",
    "    \n",
    "    # Create directory to save data files\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "\n",
    "    # Save train and test data as files\n",
    "    np.save(\"data/x_train.npy\", x_train)\n",
    "    np.save(\"data/y_train.npy\", y_train)\n",
    "    np.save(\"data/x_test.npy\", x_test)\n",
    "    np.save(\"data/y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jygNEJ4dRm3o"
   },
   "source": [
    "## Prepare The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7B7d7i3uThoq",
    "outputId": "20ca74ea-0844-4a93-c7bd-8e48d454d65d"
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    import numpy as np\n",
    "    \"\"\"Reshape the data to have 4D tensor shape (-1, 28, 28, 1).\"\"\"\n",
    "    x_train = np.load(\"data/x_train.npy\")\n",
    "    x_test = np.load(\"data/x_test.npy\")\n",
    "    \n",
    "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    \"\"\"Normalize the data by dividing it by 255.\"\"\"\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "\n",
    "    np.save(\"data/x_train.npy\", x_train)\n",
    "    np.save(\"data/x_test.npy\", x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOtAx2G9hD2W"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DI795SvBhGPW",
    "outputId": "b66e6a8c-4dba-4f0d-8675-f5bd9f3ff594"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    from tensorflow import keras\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from google.cloud import storage\n",
    "\n",
    "    \"\"\"Build the model.\"\"\"\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "        \n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    \"\"\"Compile the model.\"\"\"\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "    \n",
    "    \"\"\"Train the CNN model and save the trained model to the file system.\"\"\"\n",
    "    #load the training dataset\n",
    "    x_train = np.load(\"data/x_train.npy\")\n",
    "    y_train = np.load(\"data/y_train.npy\")\n",
    "    \n",
    "    #train the model\n",
    "    print(f\"\\n<==== model training ====>\")    \n",
    "    model.fit(x=x_train, y=y_train, epochs=1)\n",
    "    \n",
    "    # # Save the trained model\n",
    "    # if not os.path.exists(\"data/models\"):\n",
    "    #     os.makedirs(\"data/models\")\n",
    "    keras.models.save_model(model, \"/tmp/models/digits-recognizer\")\n",
    "    \n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    #load the testing dataset\n",
    "    x_test = np.load(\"data/x_test.npy\")\n",
    "    y_test = np.load(\"data/y_test.npy\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "    print(f\"Test Loss: {test_loss:.4f} || Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Upload the model to GCS; require for model serving\n",
    "    client = storage.Client()\n",
    "\n",
    "    source_directory = '/tmp/models/digits-recognizer'\n",
    "    bucket_name = 'hxyro'\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(source_directory):\n",
    "        for filename in filenames:\n",
    "            local_file_path = os.path.join(dirpath, filename)\n",
    "            relative_file_path = os.path.relpath(local_file_path, source_directory)\n",
    "            blob_name = os.path.join(\"models/digits-recognizer/1\",relative_file_path).replace(\"\\\\\", \"/\")\n",
    "            bucket = client.get_bucket(bucket_name)\n",
    "            blob = bucket.blob(blob_name)\n",
    "            blob.upload_from_filename(local_file_path)\n",
    "            print(f'Successfully uploaded {local_file_path} to GCS: {blob_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_serving():\n",
    "    \"\"\"\n",
    "    Create kserve instance\n",
    "    \"\"\"\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "\n",
    "    name='digits-recognizer'\n",
    "    api_version = constants.KSERVE_GROUP + '/v1beta1'\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version, kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                       predictor=V1beta1PredictorSpec(\n",
    "                                           tensorflow=(V1beta1TFServingSpec(storage_uri=\"gs://hxyro/models/digits-recognizer\")))))\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipelie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "\n",
    "create_step_load_data = kfp.components.create_component_from_func(\n",
    "    func=load_data,\n",
    "    base_image=\"hxyro/tensorflow-full\",\n",
    ")\n",
    "create_step_prepare_data = kfp.components.create_component_from_func(\n",
    "    func=prepare_data,\n",
    "    base_image=\"hxyro/tensorflow-full\",\n",
    ")\n",
    "create_step_build_model = kfp.components.create_component_from_func(\n",
    "    func=build_model,\n",
    "    base_image=\"hxyro/tensorflow-full\",\n",
    "    packages_to_install=['google-cloud-storage==2.8.0']\n",
    ")\n",
    "create_step_serve_model = kfp.components.create_component_from_func(\n",
    "    func=model_serving,\n",
    "    base_image=\"hxyro/tensorflow-full\",\n",
    "    packages_to_install=['kserve==0.10.1']\n",
    ")\n",
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Kubeflow Pipeline',\n",
    "   description='A sample pipeline that build and train digits-recognizer model'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def build_pipeline():\n",
    "    \n",
    "    #difine volume\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"t-vol\",\n",
    "    resource_name=\"t-vol\", \n",
    "    size=\"5Gi\",\n",
    "    modes=dsl.VOLUME_MODE_RWM)\n",
    "    \n",
    "    #difine pipeline with Dependency graph\n",
    "    load_data_task = create_step_load_data().add_pvolumes({'/data': vop.volume})\n",
    "    prepare_data_task = create_step_prepare_data().add_pvolumes({'/data': vop.volume}).after(load_data_task)\n",
    "    build_and_train_task = create_step_build_model().add_pvolumes({'/data': vop.volume}).after(prepare_data_task)\n",
    "    serve_model_task = create_step_serve_model().after(build_and_train_task)\n",
    "    \n",
    "    #remove cache (optional)\n",
    "    load_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    prepare_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    build_and_train_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    serve_model_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=build_pipeline,\n",
    "    package_path='pipeline.yaml')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "digits-recognizer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
