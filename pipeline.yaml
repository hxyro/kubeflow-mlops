apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: kubeflow-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline_compilation_time: '2023-04-16T11:14:21.221356',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "A sample pipeline that
      build and train digits-recognizer model", "name": "Kubeflow Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3}
spec:
  entrypoint: kubeflow-pipeline
  templates:
  - name: build-model
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'google-cloud-storage==2.8.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m
        pip install --quiet --no-warn-script-location 'google-cloud-storage==2.8.0'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def build_model():\n    from tensorflow import keras\n    import numpy as\
        \ np\n    import os\n    from google.cloud import storage\n\n    \"\"\"Build\
        \ the model.\"\"\"\n    model = keras.models.Sequential()\n    model.add(keras.layers.Conv2D(64,\
        \ (3, 3), activation='relu', input_shape=(28,28,1)))\n    model.add(keras.layers.MaxPool2D(2,\
        \ 2))\n\n    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n\
        \    model.add(keras.layers.MaxPool2D(2, 2))\n\n    model.add(keras.layers.Conv2D(64,\
        \ (3, 3), activation='relu'))\n    model.add(keras.layers.MaxPool2D(2, 2))\n\
        \n    model.add(keras.layers.Flatten())\n    model.add(keras.layers.Dense(64,\
        \ activation='relu'))\n\n    model.add(keras.layers.Dense(32, activation='relu'))\n\
        \n    model.add(keras.layers.Dense(10, activation='softmax'))\n\n    \"\"\"\
        Compile the model.\"\"\"\n    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\"\
        , metrics=['accuracy'])\n\n    \"\"\"Train the CNN model and save the trained\
        \ model to the file system.\"\"\"\n    #load the training dataset\n    x_train\
        \ = np.load(\"data/x_train.npy\")\n    y_train = np.load(\"data/y_train.npy\"\
        )\n\n    #train the model\n    print(f\"\\n<==== model training ====>\") \
        \   \n    model.fit(x=x_train, y=y_train, epochs=1)\n\n    # # Save the trained\
        \ model\n    # if not os.path.exists(\"data/models\"):\n    #     os.makedirs(\"\
        data/models\")\n    keras.models.save_model(model, \"/tmp/models/digits-recognizer\"\
        )\n\n    \"\"\"Evaluate the model\"\"\"\n    #load the testing dataset\n \
        \   x_test = np.load(\"data/x_test.npy\")\n    y_test = np.load(\"data/y_test.npy\"\
        )\n\n    # Evaluate the model\n    test_loss, test_acc = model.evaluate(x=x_test,\
        \ y=y_test, verbose=0)\n    print(f\"Test Loss: {test_loss:.4f} || Test Accuracy:\
        \ {test_acc:.4f}\")\n\n    # Upload the model to GCS; require for model serving\n\
        \    client = storage.Client()\n\n    source_directory = '/tmp/models/digits-recognizer'\n\
        \    bucket_name = 'hxyro'\n\n    for dirpath, dirnames, filenames in os.walk(source_directory):\n\
        \        for filename in filenames:\n            local_file_path = os.path.join(dirpath,\
        \ filename)\n            relative_file_path = os.path.relpath(local_file_path,\
        \ source_directory)\n            blob_name = os.path.join(\"models/digits-recognizer/1\"\
        ,relative_file_path).replace(\"\\\\\", \"/\")\n            bucket = client.get_bucket(bucket_name)\n\
        \            blob = bucket.blob(blob_name)\n            blob.upload_from_filename(local_file_path)\n\
        \            print(f'Successfully uploaded {local_file_path} to GCS: {blob_name}')\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Build model', description='')\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = build_model(**_parsed_args)\n"
      image: hxyro/tensorflow-full
      volumeMounts:
      - {mountPath: /data, name: t-vol}
    inputs:
      parameters:
      - {name: t-vol-name}
    metadata:
      labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp}
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''google-cloud-storage==2.8.0''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''google-cloud-storage==2.8.0'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def build_model():\n    from tensorflow
          import keras\n    import numpy as np\n    import os\n    from google.cloud
          import storage\n\n    \"\"\"Build the model.\"\"\"\n    model = keras.models.Sequential()\n    model.add(keras.layers.Conv2D(64,
          (3, 3), activation=''relu'', input_shape=(28,28,1)))\n    model.add(keras.layers.MaxPool2D(2,
          2))\n\n    model.add(keras.layers.Conv2D(64, (3, 3), activation=''relu''))\n    model.add(keras.layers.MaxPool2D(2,
          2))\n\n    model.add(keras.layers.Conv2D(64, (3, 3), activation=''relu''))\n    model.add(keras.layers.MaxPool2D(2,
          2))\n\n    model.add(keras.layers.Flatten())\n    model.add(keras.layers.Dense(64,
          activation=''relu''))\n\n    model.add(keras.layers.Dense(32, activation=''relu''))\n\n    model.add(keras.layers.Dense(10,
          activation=''softmax''))\n\n    \"\"\"Compile the model.\"\"\"\n    model.compile(optimizer=\"adam\",
          loss=\"sparse_categorical_crossentropy\", metrics=[''accuracy''])\n\n    \"\"\"Train
          the CNN model and save the trained model to the file system.\"\"\"\n    #load
          the training dataset\n    x_train = np.load(\"data/x_train.npy\")\n    y_train
          = np.load(\"data/y_train.npy\")\n\n    #train the model\n    print(f\"\\n<====
          model training ====>\")    \n    model.fit(x=x_train, y=y_train, epochs=1)\n\n    #
          # Save the trained model\n    # if not os.path.exists(\"data/models\"):\n    #     os.makedirs(\"data/models\")\n    keras.models.save_model(model,
          \"/tmp/models/digits-recognizer\")\n\n    \"\"\"Evaluate the model\"\"\"\n    #load
          the testing dataset\n    x_test = np.load(\"data/x_test.npy\")\n    y_test
          = np.load(\"data/y_test.npy\")\n\n    # Evaluate the model\n    test_loss,
          test_acc = model.evaluate(x=x_test, y=y_test, verbose=0)\n    print(f\"Test
          Loss: {test_loss:.4f} || Test Accuracy: {test_acc:.4f}\")\n\n    # Upload
          the model to GCS; require for model serving\n    client = storage.Client()\n\n    source_directory
          = ''/tmp/models/digits-recognizer''\n    bucket_name = ''hxyro''\n\n    for
          dirpath, dirnames, filenames in os.walk(source_directory):\n        for
          filename in filenames:\n            local_file_path = os.path.join(dirpath,
          filename)\n            relative_file_path = os.path.relpath(local_file_path,
          source_directory)\n            blob_name = os.path.join(\"models/digits-recognizer/1\",relative_file_path).replace(\"\\\\\",
          \"/\")\n            bucket = client.get_bucket(bucket_name)\n            blob
          = bucket.blob(blob_name)\n            blob.upload_from_filename(local_file_path)\n            print(f''Successfully
          uploaded {local_file_path} to GCS: {blob_name}'')\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Build model'', description='''')\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = build_model(**_parsed_args)\n"],
          "image": "hxyro/tensorflow-full"}}, "name": "Build model"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    volumes:
    - name: t-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.t-vol-name}}'}
  - name: kubeflow-pipeline
    dag:
      tasks:
      - name: build-model
        template: build-model
        dependencies: [prepare-data, t-vol]
        arguments:
          parameters:
          - {name: t-vol-name, value: '{{tasks.t-vol.outputs.parameters.t-vol-name}}'}
      - name: load-data
        template: load-data
        dependencies: [t-vol]
        arguments:
          parameters:
          - {name: t-vol-name, value: '{{tasks.t-vol.outputs.parameters.t-vol-name}}'}
      - name: model-serving
        template: model-serving
        dependencies: [build-model]
      - name: prepare-data
        template: prepare-data
        dependencies: [load-data, t-vol]
        arguments:
          parameters:
          - {name: t-vol-name, value: '{{tasks.t-vol.outputs.parameters.t-vol-name}}'}
      - {name: t-vol, template: t-vol}
  - name: load-data
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def load_data():
            from tensorflow import keras
            import numpy as np
            import os
            """Load MNIST dataset and save train and test data as files in the file system."""

            (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

            # check shape of the data
            print("<==== data shape ====>")
            print(f"x_train: {x_train.shape} || y_train: {y_train.shape}")
            print(f"x_test: {x_test.shape} || y_test: {y_test.shape}")

            # Create directory to save data files
            if not os.path.exists("data"):
                os.makedirs("data")

            # Save train and test data as files
            np.save("data/x_train.npy", x_train)
            np.save("data/y_train.npy", y_train)
            np.save("data/x_test.npy", x_test)
            np.save("data/y_test.npy", y_test)

        import argparse
        _parser = argparse.ArgumentParser(prog='Load data', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = load_data(**_parsed_args)
      image: hxyro/tensorflow-full
      volumeMounts:
      - {mountPath: /data, name: t-vol}
    inputs:
      parameters:
      - {name: t-vol-name}
    metadata:
      labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp}
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          load_data():\n    from tensorflow import keras\n    import numpy as np\n    import
          os\n    \"\"\"Load MNIST dataset and save train and test data as files in
          the file system.\"\"\"\n\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n    #
          check shape of the data\n    print(\"<==== data shape ====>\")\n    print(f\"x_train:
          {x_train.shape} || y_train: {y_train.shape}\")\n    print(f\"x_test: {x_test.shape}
          || y_test: {y_test.shape}\")\n\n    # Create directory to save data files\n    if
          not os.path.exists(\"data\"):\n        os.makedirs(\"data\")\n\n    # Save
          train and test data as files\n    np.save(\"data/x_train.npy\", x_train)\n    np.save(\"data/y_train.npy\",
          y_train)\n    np.save(\"data/x_test.npy\", x_test)\n    np.save(\"data/y_test.npy\",
          y_test)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Load
          data'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = load_data(**_parsed_args)\n"], "image": "hxyro/tensorflow-full"}}, "name":
          "Load data"}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    volumes:
    - name: t-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.t-vol-name}}'}
  - name: model-serving
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'kserve==0.10.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'kserve==0.10.1' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def model_serving():\n    \"\"\"\n    Create kserve instance\n    \"\"\"\n\
        \    from kubernetes import client \n    from kserve import KServeClient\n\
        \    from kserve import constants\n    from kserve import utils\n    from\
        \ kserve import V1beta1InferenceService\n    from kserve import V1beta1InferenceServiceSpec\n\
        \    from kserve import V1beta1PredictorSpec\n    from kserve import V1beta1TFServingSpec\n\
        \    from datetime import datetime\n\n    namespace = utils.get_default_target_namespace()\n\
        \n    name='digits-recognizer'\n    api_version = constants.KSERVE_GROUP +\
        \ '/v1beta1'\n\n    isvc = V1beta1InferenceService(api_version=api_version,\
        \ kind=constants.KSERVE_KIND,\n                                   metadata=client.V1ObjectMeta(name=name,\
        \ namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n\
        \                                   spec=V1beta1InferenceServiceSpec(\n  \
        \                                     predictor=V1beta1PredictorSpec(\n  \
        \                                         tensorflow=(V1beta1TFServingSpec(storage_uri=\"\
        gs://hxyro/models/digits-recognizer\")))))\n\n    KServe = KServeClient()\n\
        \    KServe.create(isvc)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Model\
        \ serving', description='Create kserve instance')\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = model_serving(**_parsed_args)\n"
      image: hxyro/tensorflow-full
    metadata:
      labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp}
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Create
          kserve instance", "implementation": {"container": {"args": [], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''kserve==0.10.1'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''kserve==0.10.1''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def model_serving():\n    \"\"\"\n    Create kserve instance\n    \"\"\"\n    from
          kubernetes import client \n    from kserve import KServeClient\n    from
          kserve import constants\n    from kserve import utils\n    from kserve import
          V1beta1InferenceService\n    from kserve import V1beta1InferenceServiceSpec\n    from
          kserve import V1beta1PredictorSpec\n    from kserve import V1beta1TFServingSpec\n    from
          datetime import datetime\n\n    namespace = utils.get_default_target_namespace()\n\n    name=''digits-recognizer''\n    api_version
          = constants.KSERVE_GROUP + ''/v1beta1''\n\n    isvc = V1beta1InferenceService(api_version=api_version,
          kind=constants.KSERVE_KIND,\n                                   metadata=client.V1ObjectMeta(name=name,
          namespace=namespace, annotations={''sidecar.istio.io/inject'':''false''}),\n                                   spec=V1beta1InferenceServiceSpec(\n                                       predictor=V1beta1PredictorSpec(\n                                           tensorflow=(V1beta1TFServingSpec(storage_uri=\"gs://hxyro/models/digits-recognizer\")))))\n\n    KServe
          = KServeClient()\n    KServe.create(isvc)\n\nimport argparse\n_parser =
          argparse.ArgumentParser(prog=''Model serving'', description=''Create kserve
          instance'')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs = model_serving(**_parsed_args)\n"],
          "image": "hxyro/tensorflow-full"}}, "name": "Model serving"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: prepare-data
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def prepare_data():
            import numpy as np
            """Reshape the data to have 4D tensor shape (-1, 28, 28, 1)."""
            x_train = np.load("data/x_train.npy")
            x_test = np.load("data/x_test.npy")

            x_train = x_train.reshape(-1, 28, 28, 1)
            x_test = x_test.reshape(-1, 28, 28, 1)

            """Normalize the data by dividing it by 255."""
            x_train = x_train / 255
            x_test = x_test / 255

            np.save("data/x_train.npy", x_train)
            np.save("data/x_test.npy", x_test)

        import argparse
        _parser = argparse.ArgumentParser(prog='Prepare data', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = prepare_data(**_parsed_args)
      image: hxyro/tensorflow-full
      volumeMounts:
      - {mountPath: /data, name: t-vol}
    inputs:
      parameters:
      - {name: t-vol-name}
    metadata:
      labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp}
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          prepare_data():\n    import numpy as np\n    \"\"\"Reshape the data to have
          4D tensor shape (-1, 28, 28, 1).\"\"\"\n    x_train = np.load(\"data/x_train.npy\")\n    x_test
          = np.load(\"data/x_test.npy\")\n\n    x_train = x_train.reshape(-1, 28,
          28, 1)\n    x_test = x_test.reshape(-1, 28, 28, 1)\n\n    \"\"\"Normalize
          the data by dividing it by 255.\"\"\"\n    x_train = x_train / 255\n    x_test
          = x_test / 255\n\n    np.save(\"data/x_train.npy\", x_train)\n    np.save(\"data/x_test.npy\",
          x_test)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Prepare
          data'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = prepare_data(**_parsed_args)\n"], "image": "hxyro/tensorflow-full"}},
          "name": "Prepare data"}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    volumes:
    - name: t-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.t-vol-name}}'}
  - name: t-vol
    resource:
      action: create
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: '{{workflow.name}}-t-vol'
        spec:
          accessModes:
          - ReadWriteMany
          resources:
            requests:
              storage: 5Gi
    outputs:
      parameters:
      - name: t-vol-manifest
        valueFrom: {jsonPath: '{}'}
      - name: t-vol-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: t-vol-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
    metadata:
      labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp}
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
